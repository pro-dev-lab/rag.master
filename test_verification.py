#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
from typing import List, Dict
import pandas as pd

def generate_manual_index(root_dir: str, manual_type: str = "user") -> List[Dict]:
    """
    ì§€ì •ëœ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ í•˜ìœ„ ë©”ë‰´ì–¼ ë””ë ‰í† ë¦¬ë¥¼ ìˆœíšŒí•˜ë©°,
    .md íŒŒì¼ê³¼ img í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ JSON í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result = []
    base_dir_name = os.path.basename(os.path.normpath(root_dir))

    for entry in os.scandir(root_dir):
        if entry.is_dir():
            manu_type = entry.name
            md_path = os.path.join(entry.path, f"{manu_type}.md")
            img_dir = os.path.join(entry.path, "img")

            if not os.path.isfile(md_path):
                continue

            image_urls = []
            if os.path.isdir(img_dir):
                for img_file in sorted(os.listdir(img_dir)):
                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                        image_urls.append(
                            f"https://doc.tg-cloud.co.kr/manual/console/{base_dir_name}/{manu_type}/img/{img_file}"
                        )

            result.append({
                "text": f"{manu_type}.md",
                "metadata": {
                    "manual_type": manual_type,
                    "manu_type": manu_type,
                    "source_url": f"https://doc.tg-cloud.co.kr/manual/console/{base_dir_name}/{manu_type}/{manu_type}",
                    "image_url": image_urls
                }
            })

    return result

def generate_chunks_from_index(index_list: List[Dict], base_dir: str) -> List[Dict]:
    """
    Markdown íŒŒì¼ì„ ì§ì ‘ ì¤„ ë‹¨ìœ„ë¡œ ì½ê³  ì„¹ì…˜(##) ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ chunk ìƒì„±
    """
    all_chunks = []

    for item in index_list:
        md_filename = item["text"]
        manu_type = item["metadata"]["manu_type"]
        manual_type = item["metadata"]["manual_type"]
        source_url = item["metadata"]["source_url"]
        image_url_list = item["metadata"]["image_url"]

        md_path = os.path.join(base_dir, manu_type, md_filename)
        if not os.path.isfile(md_path):
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {md_path}")
            continue

        with open(md_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        current_section = "__intro__"
        current_text_lines = []
        skip_section = False

        def resolve_image_urls(text: str) -> List[str]:
            matches = re.findall(r'!\[.*?\]\((.*?)\)', text)
            urls = []
            for filename in matches:
                for full_url in image_url_list:
                    if filename in full_url:
                        urls.append(full_url)
            return urls
        
        for line in lines:
            if line.strip().startswith("## "):
                if current_text_lines and not skip_section:
                    full_text = "".join(current_text_lines).strip()
                    if full_text:
                        chunk = {
                            "text": full_text,
                            "metadata": {
                                "manual_type": manual_type,
                                "manu_type": manu_type,
                                "section": current_section,
                                "source_url": source_url,
                                "image_url": resolve_image_urls(full_text)
                            }
                        }
                        all_chunks.append(chunk)

                current_section = line.strip().replace("##", "").strip()
                skip_section = (current_section == "ëª©ì°¨")
                current_text_lines = []
            else:
                if not skip_section:
                    current_text_lines.append(line)

        if current_text_lines and not skip_section:
            full_text = "".join(current_text_lines).strip()
            if full_text:
                chunk = {
                    "text": full_text,
                    "metadata": {
                        "manual_type": manual_type,
                        "manu_type": manu_type,
                        "section": current_section,
                        "source_url": source_url,
                        "image_url": resolve_image_urls(full_text)
                    }
                }
                all_chunks.append(chunk)

    return all_chunks

def main():
    print("ğŸ”„ ë§¤ë‰´ì–¼ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # 1. ë©”ë‰´ì–¼ ì¸ë±ìŠ¤ ìƒì„±
    manual_index = generate_manual_index("./manual/user/firstUser", manual_type="firstUser")
    print(f"ğŸ“‹ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(manual_index)}ê°œ ë§¤ë‰´ì–¼")
    
    # 2. ì²­í¬ ìƒì„±
    chunks = generate_chunks_from_index(manual_index, base_dir="./manual/user/firstUser")
    print(f"ğŸ“¦ ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    
    # 3. DataFrame ë³€í™˜
    df = pd.json_normalize(chunks)
    
    # 4. ê²€ì¦ ì‹œì‘
    print("\n" + "=" * 80)
    print("ğŸ“Š GENERATE_CHUNKS_FROM_INDEX í•¨ìˆ˜ ê²°ê³¼ë¬¼ ê²€ì¦")
    print("=" * 80)

    # 1. ì „ì²´ ì²­í¬ ê°œìˆ˜ ë° ê¸°ë³¸ ì •ë³´
    print(f"\n1ï¸âƒ£ ê¸°ë³¸ ì •ë³´")
    print(f"   â€¢ ì´ ì²­í¬ ê°œìˆ˜: {len(chunks)}")
    print(f"   â€¢ DataFrame í˜•íƒœ ë³€í™˜ í›„ í–‰ ìˆ˜: {len(df)}")

    # 2. ê° ì²­í¬ì˜ êµ¬ì¡° ê²€ì¦
    print(f"\n2ï¸âƒ£ ì²­í¬ êµ¬ì¡° ê²€ì¦")
    if len(chunks) > 0:
        sample_chunk = chunks[0]
        print(f"   â€¢ ì²­í¬ í‚¤: {list(sample_chunk.keys())}")
        print(f"   â€¢ ë©”íƒ€ë°ì´í„° í‚¤: {list(sample_chunk['metadata'].keys())}")

    # 3. ì„¹ì…˜ë³„ ë¶„í•  ê²€ì¦ (login.md ê¸°ì¤€)
    print(f"\n3ï¸âƒ£ ì„¹ì…˜ ë¶„í•  ê²€ì¦ (login.md ì˜ˆìƒ ê²°ê³¼)")
    login_chunks = [chunk for chunk in chunks if chunk['metadata']['manu_type'] == 'login']
    print(f"   â€¢ login ê´€ë ¨ ì²­í¬ ìˆ˜: {len(login_chunks)}")

    expected_login_sections = ["__intro__", "1. ë¡œê·¸ì¸ í˜ì´ì§€", "2. ë¡œê·¸ì¸ì„ í†µí•œ ì¸ì¦"]
    actual_login_sections = [chunk['metadata']['section'] for chunk in login_chunks]

    print(f"   â€¢ ì˜ˆìƒ ì„¹ì…˜: {expected_login_sections}")
    print(f"   â€¢ ì‹¤ì œ ì„¹ì…˜: {actual_login_sections}")
    print(f"   â€¢ ëª©ì°¨ ì„¹ì…˜ ì œì™¸ë¨: {'ëª©ì°¨' not in actual_login_sections}")

    # 4. ì´ë¯¸ì§€ URL ë§¤í•‘ ê²€ì¦
    print(f"\n4ï¸âƒ£ ì´ë¯¸ì§€ URL ë§¤í•‘ ê²€ì¦")
    for i, chunk in enumerate(login_chunks):
        section = chunk['metadata']['section']
        images = chunk['metadata']['image_url']
        text_preview = chunk['text'][:100].replace('\n', ' ')
        
        print(f"   ì²­í¬ {i+1} - ì„¹ì…˜: '{section}'")
        print(f"   â€¢ ì´ë¯¸ì§€ ê°œìˆ˜: {len(images)}")
        print(f"   â€¢ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text_preview}...")
        if images:
            print(f"   â€¢ ì²« ë²ˆì§¸ ì´ë¯¸ì§€: {images[0]}")
        print()

    # 5. namespaces.md ê²€ì¦
    print(f"\n5ï¸âƒ£ namespaces.md ê²€ì¦")
    namespace_chunks = [chunk for chunk in chunks if chunk['metadata']['manu_type'] == 'namespaces']
    print(f"   â€¢ namespaces ê´€ë ¨ ì²­í¬ ìˆ˜: {len(namespace_chunks)}")

    expected_namespace_sections = ["__intro__", "1. Namespace ë©”ë‰´ ì§„ì…", "2. Namespace ìƒì„± ì‹ ì²­", "3. Namespace ìƒì„± í™•ì¸"]
    actual_namespace_sections = [chunk['metadata']['section'] for chunk in namespace_chunks]

    print(f"   â€¢ ì˜ˆìƒ ì„¹ì…˜: {expected_namespace_sections}")
    print(f"   â€¢ ì‹¤ì œ ì„¹ì…˜: {actual_namespace_sections}")

    # 6. ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
    print(f"\n6ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ê²€ì¦")
    manual_types = set(chunk['metadata']['manual_type'] for chunk in chunks)
    manu_types = set(chunk['metadata']['manu_type'] for chunk in chunks)

    print(f"   â€¢ manual_type ì¢…ë¥˜: {manual_types}")
    print(f"   â€¢ manu_type ì¢…ë¥˜: {manu_types}")
    print(f"   â€¢ ëª¨ë“  ì²­í¬ê°€ source_url ë³´ìœ : {all('source_url' in chunk['metadata'] for chunk in chunks)}")

    # 7. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    print(f"\n7ï¸âƒ£ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„")
    text_lengths = [len(chunk['text']) for chunk in chunks]
    print(f"   â€¢ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {sum(text_lengths)/len(text_lengths):.1f} ë¬¸ì")
    print(f"   â€¢ ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {min(text_lengths)} ë¬¸ì")
    print(f"   â€¢ ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´: {max(text_lengths)} ë¬¸ì")

    # 8. ë¹ˆ ì²­í¬ ë˜ëŠ” ë¬¸ì œ ì²­í¬ ê²€ì¦
    print(f"\n8ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    empty_chunks = [chunk for chunk in chunks if not chunk['text'].strip()]
    print(f"   â€¢ ë¹ˆ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(empty_chunks)}")

    missing_metadata_chunks = [chunk for chunk in chunks if not all(key in chunk['metadata'] for key in ['manual_type', 'manu_type', 'section', 'source_url'])]
    print(f"   â€¢ ë©”íƒ€ë°ì´í„° ëˆ„ë½ ì²­í¬ ìˆ˜: {len(missing_metadata_chunks)}")

    print(f"\n{'='*80}")
    print("âœ… ê²€ì¦ ì™„ë£Œ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    main() 