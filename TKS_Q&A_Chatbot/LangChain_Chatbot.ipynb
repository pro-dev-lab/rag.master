{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23831c22",
   "metadata": {},
   "source": [
    "### VectorDB -> ë²¡í„°ë¼ì´ì§• ë°ì´í„° í™œìš© -> LAGChain êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6874b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG í”„ë¡œì íŠ¸ ì¶œë ¥ ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¥ ê°„ë‹¨í•œ ì„¤ì •\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# RAG í”„ë¡œì íŠ¸ìš© ê¶Œì¥ ì„¤ì •\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ ì œí•œ í•´ì œ\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ë²¡í„° ì¶œë ¥ ì œí•œ í•´ì œ\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "print(\"RAG í”„ë¡œì íŠ¸ ì¶œë ¥ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# íŠ¹ì • ì¶œë ¥ì—ì„œë§Œ ì „ì²´ í‘œì‹œ\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(your_dataframe)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ì„ì‹œë¡œ ì„¤ì • ë³€ê²½\n",
    "old_max_rows = pd.get_option('display.max_rows')\n",
    "old_max_columns = pd.get_option('display.max_columns')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#print(your_dataframe)\n",
    "\n",
    "# ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›\n",
    "pd.set_option('display.max_rows', old_max_rows)\n",
    "pd.set_option('display.max_columns', old_max_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc91aa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python-dotenv íŒ¨í‚¤ì§€: í™˜ê²½ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# - ë³´ì•ˆì´ í•„ìš”í•œ API í‚¤ ë“±ì„ ê´€ë¦¬í•˜ëŠ”ë° ì‚¬ìš©\n",
    "# - pip install python-dotenvë¡œ ì„¤ì¹˜ ê°€ëŠ¥\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv(): .env íŒŒì¼ì˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ í˜„ì¬ ì‹¤í–‰ í™˜ê²½ì— ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "# - ë°˜í™˜ê°’: ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0019ac",
   "metadata": {},
   "source": [
    "### Prompt - ë²„ì „ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "92074409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ì‚¬ìš© ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n",
    "====================\n",
    "{context}\n",
    "====================\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb60ad",
   "metadata": {},
   "source": [
    "### Prompt - ë²„ì „ 1.5 (ì¶œë ¥ê°’ì„ ì •ë ¬ ë° ì‚¬ëŒì´ ë³´ê³  í¸í•œ êµ¬ì¡°ë¡œ ê°œì„ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7799838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ì‚¬ìš© ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n",
    "====================\n",
    "{context}\n",
    "====================\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "1. ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.\n",
    "2. ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•˜ë©´ **ìˆ«ì ëª©ë¡**ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "3. UI ê²½ë¡œëŠ” `\"Menu > Submenu\"` í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.\n",
    "4. ì „ì²´ ë‹µë³€ì€ 500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "5. ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "ë‹µë³€:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788dbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3k/glrtc_n92r1b88148508h8rc0000gn/T/ipykernel_86314/4021498024.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "/var/folders/3k/glrtc_n92r1b88148508h8rc0000gn/T/ipykernel_86314/4021498024.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì»¬ë ‰ì…˜ ì´ë¦„: manual_user_collection\n",
      "ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜: 97\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ChromaDB ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°\n",
    "# persist_directoryëŠ” ChromaDBê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# ê¸°ì¡´ ì»¬ë ‰ì…˜ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"manual_user_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ì»¬ë ‰ì…˜ ì´ë¦„: {vectorstore._collection.name}\")\n",
    "print(f\"ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ee6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def parse_image_urls(image_urls_raw):\n",
    "    if isinstance(image_urls_raw, str):\n",
    "        try:\n",
    "            image_urls = json.loads(image_urls_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            image_urls = []\n",
    "    elif isinstance(image_urls_raw, list):\n",
    "        image_urls = image_urls_raw\n",
    "    else:\n",
    "        image_urls = []\n",
    "    return image_urls\n",
    "\n",
    "def format_images(image_urls):\n",
    "    return \"\\n\".join([f\"- ![image]({url})\" for url in image_urls])\n",
    "\n",
    "def format_result_with_metadata(question: str, answer: str, docs: List[Document]):\n",
    "    md = f\"### ğŸ’¬ ì§ˆë¬¸\\n{question.strip()}\\n\\n\"\n",
    "    md += f\"### ğŸ§  ë‹µë³€\\n{answer.strip()}\\n\\n\"\n",
    "    md += \"---\\n\\n\"\n",
    "    md += f\"### ğŸ“ ê´€ë ¨ ë¬¸ì„œ ë° ì´ë¯¸ì§€\\n\"\n",
    "\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        url = doc.metadata.get('source_url', '')\n",
    "        image_urls_raw = doc.metadata.get('image_urls', '[]')\n",
    "\n",
    "        md += f\"\\n**ğŸ”— ë¬¸ì„œ ì¶œì²˜ {i}**\\n\"\n",
    "        md += f\"[ì›¹ ë©”ë‰´ì–¼ ë³´ê¸°]({url})\\n\"\n",
    "\n",
    "        image_urls = parse_image_urls(image_urls_raw)\n",
    "        if image_urls:\n",
    "            markdown_images = format_images(image_urls[:3])\n",
    "            md += \"ì´ë¯¸ì§€:\\n\"\n",
    "            md += markdown_images + \"\\n\"\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2a634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ì„ ì§€ì • ê°€ëŠ¥\n",
    "            temperature=0,        # temperatureëŠ” 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€ì„, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "            max_tokens=100,       # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "            )\n",
    "\n",
    "# Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a306ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹µë³€: ### ğŸ’¬ ì§ˆë¬¸\n",
      "ë¡œê·¸ì¸ì€ ì–´ë–»ê²Œ í•´ì•¼í•´?\n",
      "\n",
      "### ğŸ§  ë‹µë³€\n",
      "## ë¡œê·¸ì¸ ë°©ë²•\n",
      "\n",
      "ë¡œê·¸ì¸ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë¸Œë¼ìš°ì € ì ‘ì†**\n",
      "   - ë¸Œë¼ìš°ì € URL ì°½ì— `httpstkctl.tg-cloud.co.kr`ì„ ì…ë ¥í•˜ì—¬ í¬íƒˆì— ì ‘ì†í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ë¡œê·¸ì¸ ì¸ì¦**\n",
      "   - ë¡œê·¸ì¸ ì°½ì—ì„œ IDì™€ PWë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\n",
      "     - **ID**: username ë˜ëŠ” email í˜•ì‹ìœ¼ë¡œ ì…ë ¥\n",
      "     - **PW**: ë¡œê·¸ì¸ ê³„ì •ì˜ ë¹„ë°€ë²ˆí˜¸\n",
      "\n",
      "3. **í¬\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“ ê´€ë ¨ ë¬¸ì„œ ë° ì´ë¯¸ì§€\n",
      "\n",
      "**ğŸ”— ë¬¸ì„œ ì¶œì²˜ 1**\n",
      "[ì›¹ ë©”ë‰´ì–¼ ë³´ê¸°](https://doc.tg-cloud.co.kr/manual/console/login/login/login)\n",
      "ì´ë¯¸ì§€:\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/login/login/img/login_url.png)\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/login/login/img/login_input.png)\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/login/login/img/login_success.png)\n",
      "\n",
      "**ğŸ”— ë¬¸ì„œ ì¶œì²˜ 2**\n",
      "[ì›¹ ë©”ë‰´ì–¼ ë³´ê¸°](https://doc.tg-cloud.co.kr/manual/console/firstUser/login/login)\n",
      "ì´ë¯¸ì§€:\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/firstUser/login/img/login_url.png)\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/firstUser/login/img/login_input.png)\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/firstUser/login/img/login_first_menu.png)\n",
      "\n",
      "**ğŸ”— ë¬¸ì„œ ì¶œì²˜ 3**\n",
      "[ì›¹ ë©”ë‰´ì–¼ ë³´ê¸°](https://doc.tg-cloud.co.kr/manual/console/header/header/header)\n",
      "ì´ë¯¸ì§€:\n",
      "- ![image](https://doc.tg-cloud.co.kr/manual/console/header/header/img/header_info.png)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜ (List[Document] íƒ€ì…)\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "#retriever_docs_backup = []\n",
    "retriever_docs_backup: List[Document] = []\n",
    "# section í•„ë“œë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_section(docs):\n",
    "    global retriever_docs_backup\n",
    "    retriever_docs_backup = docs\n",
    "    return {\"context\": [doc.page_content for doc in docs]}\n",
    "\n",
    "\n",
    "# LCEL ì²´ì¸ êµ¬ì„±\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(extract_section), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì²´ì¸ í…ŒìŠ¤íŠ¸\n",
    "question = \"ë¡œê·¸ì¸ì€ ì–´ë–»ê²Œ í•´ì•¼í•´?\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "# ê²°ê³¼ í¬ë§·íŒ…(LLM ì¶œë ¥ + ë²¡í„°DB ë©”íƒ€ë°ì´í„°)\n",
    "final_output = format_result_with_metadata(question, response, retriever_docs_backup)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# display(Markdown(final_output))\n",
    "print(\"ë‹µë³€:\", final_output)\n",
    "\n",
    "with open(f\"output_{question}.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_output)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"ì§ˆë¬¸:\", question)\n",
    "#print(\"ë‹µë³€:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: TypeError - expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.documents import Document  # ê¼­ í•„ìš”!\n",
    "\n",
    "def parse_image_urls(image_urls_raw):\n",
    "    # 1. ë¬¸ìì—´ì¼ ê²½ìš° â†’ json.loads\n",
    "    if isinstance(image_urls_raw, str):\n",
    "        try:\n",
    "            return json.loads(image_urls_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "    # 2. ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "    elif isinstance(image_urls_raw, list):\n",
    "        return image_urls_raw\n",
    "\n",
    "    # 3. ê·¸ ì™¸ (NoneType, dict ë“±) â†’ ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "    return []\n",
    "\n",
    "\n",
    "# ğŸ§¾ ì‘ë‹µ + ë©”íƒ€ë°ì´í„° ë§ˆí¬ë‹¤ìš´ í¬ë§·\n",
    "def format_result_with_metadata(question: str, answer: str, docs: List[Document]):\n",
    "    md = f\"### ğŸ’¬ ì§ˆë¬¸\\n{question.strip()}\\n\\n\"\n",
    "    md += f\"### ğŸ§  ë‹µë³€\\n{answer.strip()}\\n\\n\"\n",
    "    md += \"---\\n\\n\"\n",
    "    md += f\"### ğŸ“ ê´€ë ¨ ë¬¸ì„œ ë° ì´ë¯¸ì§€\\n\"\n",
    "\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        url = doc.metadata.get('source_url', '#')\n",
    "        image_urls_raw = doc.metadata.get('image_urls', [])\n",
    "\n",
    "        md += f\"\\n**ğŸ”— ë¬¸ì„œ ì¶œì²˜ {i}**\\n\"\n",
    "        md += f\"[ì›¹ ë©”ë‰´ì–¼ ë³´ê¸°]({url})\\n\"\n",
    "\n",
    "        image_urls = parse_image_urls(image_urls_raw)\n",
    "        if image_urls:\n",
    "            md += \"ì´ë¯¸ì§€:\\n\"\n",
    "            for img in image_urls[:3]:  # ìµœëŒ€ 3ê°œ ì¶œë ¥\n",
    "                md += f\"- ![image]({img})\\n\"\n",
    "\n",
    "    return md\n",
    "\n",
    "# ğŸ§  ì‘ë‹µ ìƒì„± ì²´ì¸ í•¨ìˆ˜ (ì˜ˆì™¸ì²˜ë¦¬ í¬í•¨)\n",
    "def generate_response(user_input):\n",
    "    question = str(user_input).strip()\n",
    "\n",
    "    if not question:\n",
    "        return \"âŒ ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "    try:\n",
    "        # ğŸ” ì‹¤ì œ ì½”ë“œì—ì„œëŠ” ì™¸ë¶€ ì •ì˜ëœ ê°ì²´ ì‚¬ìš©\n",
    "        docs = retriever.invoke(question)\n",
    "        context = extract_section(docs)\n",
    "        answer = chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": context[\"context\"]\n",
    "        })\n",
    "        return format_result_with_metadata(question, answer, docs)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response_md = generate_response(\"TKSCTLë¡œ í„°ë¯¸ë„ ì ‘ì†í•˜ëŠ” ë²• ì•Œë ¤ì¤˜\")\n",
    "print(response_md)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±\n",
    "# with gr.Blocks(title=\"ë¬¸ì„œ ê¸°ë°˜ QA ë´‡\") as demo:\n",
    "#     gr.Markdown(\"## ğŸ“˜ ë¬¸ì„œ ê¸°ë°˜ QA ì±—ë´‡\\nì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "#     with gr.Row():\n",
    "#         txt_input = gr.Textbox(lines=2, label=\"ì§ˆë¬¸ ì…ë ¥\")\n",
    "#         submit_btn = gr.Button(\"ì§ˆë¬¸í•˜ê¸°\")\n",
    "\n",
    "#     output_area = gr.Markdown(label=\"ğŸ“‹ ë‹µë³€ ê²°ê³¼\")\n",
    "\n",
    "#     submit_btn.click(\n",
    "#         fn=generate_response,\n",
    "#         inputs=txt_input,\n",
    "#         outputs=output_area\n",
    "#     )\n",
    "\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d10e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7864\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc286e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
