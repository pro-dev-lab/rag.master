{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc91aa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python-dotenv 패키지: 환경 변수를 .env 파일에서 로드하는 라이브러리\n",
    "# - 보안이 필요한 API 키 등을 관리하는데 사용\n",
    "# - pip install python-dotenv로 설치 가능\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv(): .env 파일의 환경 변수를 현재 실행 환경에 로드하는 함수\n",
    "# - 반환값: 성공 시 True, 실패 시 False\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bee7ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "당신은 클라우드 사용 매뉴얼을 기반으로 질문에 답변하는 전문가입니다.\n",
    "\n",
    "다음은 검색된 문서 내용입니다:\n",
    "====================\n",
    "{context}\n",
    "====================\n",
    "\n",
    "위 내용을 참고하여 다음 질문에 정확하고 간결하게 답변하세요:\n",
    "질문: {question}\n",
    "\n",
    "- 검색된 문서의 내용을 벗어나지 마세요.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7799838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "당신은 클라우드 사용 매뉴얼을 기반으로 질문에 답변하는 전문가입니다.\n",
    "\n",
    "다음은 검색된 문서 내용입니다:\n",
    "====================\n",
    "{context}\n",
    "====================\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 내용을 참고하여 다음 조건을 지켜 답변하세요:\n",
    "\n",
    "1. 반드시 마크다운 형식으로 정리하세요.\n",
    "2. 단계별 설명이 필요하면 **숫자 목록**으로 나누어 설명하세요.\n",
    "3. UI 경로는 `\"Menu > Submenu\"` 형식으로 표현하세요.\n",
    "4. 전체 답변은 500자 이내로 간결하게 작성하세요.\n",
    "5. 검색된 문서의 내용을 벗어나지 마세요.\n",
    "\n",
    "답변:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "당신은 클라우드 사용 매뉴얼을 기반으로 질문에 답변하는 전문가입니다.\n",
    "\n",
    "다음은 검색된 문서 내용입니다:\n",
    "====================\n",
    "{context}\n",
    "====================\n",
    "\n",
    "위 내용을 참고하여 다음 질문에 정확하고 간결하게 답변하세요:\n",
    "질문: {question}\n",
    "\n",
    "- 검색된 문서의 내용을 벗어나지 마세요.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788dbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3k/glrtc_n92r1b88148508h8rc0000gn/T/ipykernel_65416/4021498024.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "/var/folders/3k/glrtc_n92r1b88148508h8rc0000gn/T/ipykernel_65416/4021498024.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터스토어가 성공적으로 로드되었습니다.\n",
      "컬렉션 이름: manual_user_collection\n",
      "컬렉션 내 문서 수: 97\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ChromaDB 벡터스토어 연결\n",
    "# persist_directory는 ChromaDB가 저장된 디렉토리 경로\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# 기존 컬렉션에서 벡터스토어 로드\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"manual_user_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"벡터스토어가 성공적으로 로드되었습니다.\")\n",
    "print(f\"컬렉션 이름: {vectorstore._collection.name}\")\n",
    "print(f\"컬렉션 내 문서 수: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e2a634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 네임스페이스 목록을 확인하고 싶어.\n",
      "답변: 네임스페이스 목록을 확인하는 방법은 다음과 같습니다:\n",
      "\n",
      "1. **Namespace 메뉴 진입**\n",
      "   - `좌측 메뉴 > Namespaces` 메뉴를 클릭합니다.\n",
      "   - 네임스페이스 화면에 진입하면, 현재 할당된 네임스페이스가 없을 경우 빈 리스트로 표시됩니다.\n",
      "\n",
      "2. **네임스페이스 생성 확인**\n",
      "   - 네임스페이스가 생성된 후, `신청관리 화면`에서\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",  # 사용할 모델 이름을 지정 가능\n",
    "            temperature=0,        # temperature는 0~1 사이의 값으로, 0에 가까울수록 일관된 답변을, 1에 가까울수록 다양하고 창의적인 답변을 생성합니다\n",
    "            max_tokens=100,       # 생성할 최대 토큰 수\n",
    "            )\n",
    "\n",
    "# Retriever 생성\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "\n",
    "retriever_docs_backup = []\n",
    "# section 필드만 추출하는 함수\n",
    "def extract_section(docs):\n",
    "    global retriever_docs_backup\n",
    "    retriever_docs_backup = docs\n",
    "    return {\"context\": [doc.page_content for doc in docs]}\n",
    "\n",
    "\n",
    "# # 검색 쿼리 수행\n",
    "# docs = retriever.invoke(\"네임스페이스 목록을 확인하고 싶어?\")\n",
    "\n",
    "# result = extract_section(docs)\n",
    "# print(\"추출된 섹션 결과:\", result)\n",
    "# print()\n",
    "\n",
    "# # 검색 결과 출력\n",
    "# for i, doc in enumerate(docs):\n",
    "#     print(f\"\\n--- 검색 결과 {i+1} ---\")\n",
    "#     print(f\"내용: {doc.page_content}\")\n",
    "#     print(f\"메타데이터: {doc.metadata}\")    \n",
    "# print('--------------------------------')\n",
    "\n",
    "\n",
    "# for i, doc in enumerate(retriever_docs_backup):\n",
    "#     print(f\"\\n--- 검색 결과 {i+1} ---\")\n",
    "#     print(f\"내용: {doc.page_content}\")\n",
    "#     print(f\"메타데이터: {doc.metadata}\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LCEL 체인 구성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 체인 테스트\n",
    "question = \"네임스페이스 목록을 확인하고 싶어.\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", response)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08bcead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21798086",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64c25cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gu.han/Documents/AI.WORK/RAG_Master/langchain_env/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio는 머신러닝 모델을 위한 웹 인터페이스를 쉽게 만들 수 있게 해주는 파이썬 라이브러리입니다.\n",
    "# 주요 특징:\n",
    "# - 간단한 코드로 대화형 UI 생성 가능\n",
    "# - 다양한 입출력 컴포넌트 제공 (텍스트, 이미지, 오디오 등)\n",
    "# - 로컬 호스팅 및 Hugging Face Spaces 배포 지원\n",
    "import gradio as gr\n",
    "\n",
    "def answer_invoke(message, history):\n",
    "    response = chain.invoke({\"question\": message})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "\n",
    "# Graiio 인터페이스 생성 \n",
    "# Gradio의 ChatInterface 클래스를 사용하여 챗봇 인터페이스를 생성합니다\n",
    "# - fn=answer_invoke: 사용자 입력을 처리할 콜백 함수를 지정합니다\n",
    "# - title=\"QA Bot\": 챗봇 UI의 제목을 설정합니다\n",
    "demo = gr.ChatInterface(fn=answer_invoke, title=\"QA Bot\")\n",
    "\n",
    "# Graiio 실행  \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d10e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc286e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
